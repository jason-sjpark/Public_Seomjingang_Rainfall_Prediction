{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5XCWvCsv8dAB",
    "outputId": "67f48b08-e15d-4034-a0dc-5b2ce36e1504",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5_aI5U_a8nr1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# import package\n",
    "from __future__ import print_function, division\n",
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from torchsummary import summary\n",
    "from torch import optim\n",
    "\n",
    "# dataset and transformation\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, utils\n",
    "import os\n",
    "\n",
    "# display images\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# utils for writing own dataset\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "IMG_SIZE  = (224,224) # image size of efficient net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc4F5GMW9OkK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. 데이터셋 불러오기\n",
    "데이터셋은 torchvision 패키지에서 제공하는 STL10 dataset을 이용하겠습니다. STL10 dataset은 10개의 label을 갖으며 train dataset 5000개, test dataset 8000개로 구성됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10DyLx_7WCRQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "WRITING CUSTOM DATASETS, DATALOADERS AND TRANSFORMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "earuwLSiWvU0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reading Annotations (groud truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Fgex1RYWuW6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# specify path to data\n",
    "path2data = 'data/train/'\n",
    "\n",
    "train_gt_data_all = pd.read_csv(path2data + 'new_train_output_seomjingang.csv')\n",
    "\n",
    "# get all timestamps\n",
    "gt_timestamps = train_gt_data_all['관측시간']\n",
    "gt_timestamps = list(dict.fromkeys(gt_timestamps)) # remove duplicates\n",
    "gt_timestamps = [gt_timestamps[i].replace('-', '').replace(' ', '') + '00' for i in range(len(gt_timestamps))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtZhO1d3v734",
    "outputId": "5a45a75a-ec5b-468b-9159-ec2930f8f6bb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(gt_timestamps[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTsZ0vbchAmq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get GPM, RR, TPW\n",
    "import xarray as xr\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "filenames_dict = dict() # {timestamp: [list of files]}\n",
    "\n",
    "def filter_by_timestamp(original_list, f_dict):\n",
    "    new_list = []\n",
    "    for timestamp in gt_timestamps:\n",
    "        for i in range(len(original_list)):\n",
    "            if timestamp in original_list[i]:\n",
    "                new_list.append(original_list[i])\n",
    "                if timestamp not in f_dict.keys():\n",
    "                    f_dict[timestamp] = [original_list[i]]\n",
    "                else:\n",
    "                    f_dict[timestamp].append(original_list[i])\n",
    "                break\n",
    "    return new_list\n",
    "\n",
    "def gpm_filter_by_timestamp(original_list, f_dict):\n",
    "    new_list = []\n",
    "    for timestamp in gt_timestamps:\n",
    "        for i in range(len(original_list)):\n",
    "            if timestamp[0:8] in original_list[i] and 'S'+timestamp[8:13] in original_list[i] :\n",
    "                new_list.append(original_list[i])\n",
    "                if timestamp not in f_dict.keys():\n",
    "                    f_dict[timestamp] = [original_list[i]]\n",
    "                else:\n",
    "                    f_dict[timestamp].append(original_list[i])\n",
    "                break\n",
    "    return new_list\n",
    "\n",
    "# GPM\n",
    "#!pip install rasterio\n",
    "gpm_file_list = glob.glob(\"data/train/input_nasa_gpm/*.tif\")\n",
    "# only retrive files matching the timestamp\n",
    "gpm_file_list = gpm_filter_by_timestamp(gpm_file_list, filenames_dict)\n",
    "#gpm_dset_all = [xr.open_rasterio(gpm_file_list[i]) for i in range(len(gpm_file_list))]\n",
    "#gpm_images = [gpm_dset_all[i].to_dataset('band').rename({1: 'red'}).red.values for i in range(len(gpm_file_list))]\n",
    "\n",
    "# TPW \n",
    "# get all *.nc files\n",
    "valid_max = 10000\n",
    "tpw_file_list = glob.glob(\"E:\\ai_factory\\tpw\\work*.nc\")\n",
    "tpw_file_list = filter_by_timestamp(tpw_file_list, filenames_dict)\n",
    "#tpw_dset_all = [xr.open_dataset(tpw_file_list[i]) for i in range(len(tpw_file_list))]\n",
    "#tpw_images = [tpw_dset_all[i]['TPW'].values / valid_max for i in range(len(tpw_dset_all))]\n",
    "\n",
    "# RR \n",
    "# get all *.nc files\n",
    "rr_file_list = glob.glob(\"E:\\ai_factory\\tpw\\work*.nc\")\n",
    "rr_file_list = filter_by_timestamp(rr_file_list, filenames_dict)\n",
    "#rr_dset_all = [xr.open_dataset(rr_file_list[i]) for i in range(len(rr_file_list))]\n",
    "#rr_images = [rr_dset_all[i]['RR'].values / valid_max for i in range(len(rr_dset_all))]\n",
    "\n",
    "#train_file_list_only_names = [train_file_list[i][train_file_list[i].find(\"train/\")+6:] for i in range(len(train_file_list))]\n",
    "#print(train_file_list_only_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1B536hydMTyz",
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to a single image\n",
    "time_image_dict = dict()\n",
    "\n",
    "IMG_SIZE  = (224, 224) # image size of efficient net\n",
    "valid_max = 100.\n",
    "scale = 10.\n",
    "train_images = []\n",
    "for k in filenames_dict.keys():\n",
    "    img_filename_list = filenames_dict[k]\n",
    "    if (len(img_filename_list) == 3):\n",
    "        gpm_img = np.array([])\n",
    "        tpw_img = np.array([])\n",
    "        rr_img = np.array([])\n",
    "        for filename in img_filename_list:\n",
    "            if 'HHR' in filename:\n",
    "                temp = xr.open_rasterio(filename).to_dataset('band').rename({1: 'red'}).red.values * 10\n",
    "                gpm_img = temp.copy()\n",
    "                gpm_img.resize(IMG_SIZE)\n",
    "            if 'tpw' in filename:\n",
    "                temp = xr.open_dataset(filename)['TPW'].values / valid_max\n",
    "                tpw_img = temp.copy()\n",
    "                tpw_img.resize(IMG_SIZE)\n",
    "            if 'rr' in filename:\n",
    "                temp = xr.open_dataset(filename)['RR'].values / valid_max\n",
    "                rr_img = temp.copy()\n",
    "                rr_img.resize(IMG_SIZE)\n",
    "        #print(\"gpm size: \", gpm_img.shape)\n",
    "        #print(\"tpw size: \", tpw_img.shape)\n",
    "        #print(\"rr size: \", rr_img.shape)\n",
    "        stacked_image = np.dstack((tpw_img, rr_img, gpm_img))\n",
    "        time_image_dict[k] = stacked_image\n",
    "        train_images.append(stacked_image)\n",
    "#print(time_image_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7mVztvoLM8b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save converted files to .npy\n",
    "save_as_npy = False\n",
    "if (save_as_npy):\n",
    "    path2data = '/content/gdrive/MyDrive/data/'\n",
    "    with open(path2data + 'gpm_images.npy', 'wb') as f:\n",
    "        np.save(f, gpm_images)\n",
    "    with open(path2data + 'tpw_images.npy', 'wb') as f:\n",
    "        np.save(f, tpw_images)\n",
    "    with open(path2data + 'rr_images.npy', 'wb') as f:\n",
    "        np.save(f, rr_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rz_VuasSwvy0",
    "outputId": "7f880adb-68a1-4060-eb28-e4e677387a3b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load from npy\n",
    "time_image_dict = np.load('C:/Users/GomZoo/Downloads/rgb_images.npy', allow_pickle=True)\n",
    "time_image_dict = time_image_dict[()]\n",
    "type(time_image_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gl-kSa88ZE-m",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Data structure is as follows:\n",
    "image_name, 년월일시, 도통리 시강수량, 용암리 시강수량, 시산리 시강수량, 섬진강댐 시강수량, 복내리 시강수량, 주암댐 시강수량, 동가리 시강수량, 맹리 시강수량, 우산리 시강수량, 동복댐 시강수량, 봉동리 시강수량, 보성강댐 시강수량\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0qKa5QQrbzoo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SeomjingangDataset(Dataset):\n",
    "    \"\"\"Seomjingang dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, time_image_dict, transform=None):\n",
    "        \"\"\"\n",
    "        Converts csv data into N x 12 (no of observatories) ordered by time with image names added\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            n (int): Number of data entries to use.\n",
    "            image_names (list): List of file names (timestamps) that will be added to the ground truth data\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.train_gt_data_all = pd.read_csv(root_dir + \"/\" + csv_file)\n",
    "        #self.train_gt_data = train_gt_data_all.head(n)\n",
    "        print(\"image_dict length\", len(time_image_dict.keys()))\n",
    "        self.train_data = self.train_gt_data_all.pivot(index=\"관측시간\", columns=\"관측소명\", values=\"시강수량\")\n",
    "        print(\"image_dict length\", len(self.train_data))\n",
    "        self.train_data = self.train_data.head(len(time_image_dict.keys()))\n",
    "        self.train_data.insert(0, \"image_name\", time_image_dict.keys())\n",
    "        self.root_dir = root_dir\n",
    "        self.time_image_dict = time_image_dict\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        image_name = self.train_data.iloc[idx, 0]\n",
    "        tpw_image = self.time_image_dict[image_name]\n",
    "        tpws = self.train_data.iloc[idx, 1:]\n",
    "        tpws = np.array([tpws])\n",
    "        tpws = tpws.astype('float')\n",
    "        sample = {'image': tpw_image, 'hourly_rainfall': tpws}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "        \n",
    "    def convertNC2img(self, nc_file_name):\n",
    "        max_tpw = 10000 # 10000 is the max_tpw\n",
    "        #nc_files_all = [xr.open_dataset(self.root_dir + nc_file_list[i]) for i in range(len(nc_file_list))]\n",
    "        #tpw_images = [nc_files_all[i]['TPW'].values / max_tpw for i in range(len(nc_file_list))] \n",
    "        nc_file = xr.open_dataset(self.root_dir + \"/\" + nc_file_name)\n",
    "        tpw_image = nc_file['TPW'].values / max_tpw \n",
    "        return tpw_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "id": "afBs5mmHoM9Z",
    "outputId": "0965c5f7-d34f-4780-8d51-046d30d42b21",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_dict length 11498\n",
      "image_dict length 11664\n",
      "image_dict length 11498\n",
      "image_dict length 11664\n",
      "<__main__.SeomjingangDataset object at 0x0000012DD7EBCB00>\n",
      "0 (224, 224, 3) (1, 12)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (224, 224, 3) (1, 12)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABrCAYAAAAYTP40AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAKP0lEQVR4nO2dX6hdRxXGf9+9Sa1WQ6rWoGlt/C+ohVrFIhX7YCkKxRd9EPqgL4rgg1AKIootFPRBJOCjSiNGqj5UEa3SavEPSis1Yi1VodBKaKXGtkljjCHJXT7s2ffOmTN775m9Z/a90fPByd3n7NlrZr5Zs2atNXNOZGasMA/WtrsB/09YkT0jVmTPiBXZM2JF9oxYkT0jLniyJd0m6fB2tyMFo8mWdJ2k30o6IelZSb+R9M6SjZsTkn4n6Q2SXivpSHDvU5IeknRG0qGxdewa2bA9wI+ATwLfAy4C3gOcGduQ7YSk3cCVwGPAh4AjQZGngDuAG4EXjq1nrGa/EcDM7jKz82Z22szuNbOHASS9TtL9kp6R9E9J35a0t31Y0hOSbpX0sKRTkr4haZ+kn0g6Kelnki51ZQ9IMkkfl/SUpL9LuqWrYZKudTPuuKQ/Sro+oT9vBR61Jpx+BwHZZna3mf0AeCaLpRBmlv0C9riKvwm8H7g0uP964AbgBcBlwK+Ag979J4AHgH3AfuAfroNXu2fuB77gyh4ADLgLuAR4G3AMeJ+7fxtw2F3vd+36AI0i3eDeX9bRj48Bx4F/A/9x1+eAk+76NUH5O4BDYzgzs3GabWbPA9c5Er4GHJP0Q0n73P3HzOw+MztjZseArwDvDcR81cyeNrMngV8DD5rZH8zsDPB9R7yP283slJn9CbgT+EikaTcD95jZPWa2YWb3AQ/RkB/rx51mthf4PXAtcBXwCLDHzPaa2eN5zPRj9AJpZn82s4+a2eU00/BVwEEASa+Q9B1JT0p6HjgMvDwQ8bR3fTry/sVB+aPe9d9cfSGuBD7sTMhxScdplOKVYUFJL3VlTgDvBn4B/BV4E/CcpE939X0sirh+ZvYX4BAN6QBfpNH6q8xsD43GaWI1V3jXr6ZZtEIcBb7ltLJ9XWJmX4q0+Vmn1Z8Avu6ufwrc5J47OLG9SxhFtqQ3S7pF0uXu/RU00/oBV+QlwL+A45L2A7cWaOvnJb1I0ltobO13I2UOAzdJulHSuqSLJV3ftrMD17C1IF5NY1IWIGmXpIuBdaCVm+3JjdXsk8C7gAclnaIh+RGg9RJuB94OnAB+DNw9sh4fv6RxzX4OfNnM7g0LmNlR4IPAZ2kW0aM0A93Xz2uAI5JeBpw3s+ciZT5HY9o+QzNLT7vPsqCdvnkg6QDwOLDbzM5tc3Mm4YIP1y8krMieETvejPwvYaXZM2JF9ozo9RV3SQbTo5EhiCYC2g7UqPecWZSyJMe8q0GlBmFOordzhRqXz55a65p7bbjXjGjbvh2kZ5NdRJsdyRLLNkQwh4MU64d592o0IZvsgJdh9LTcwnvtcj2j2sWqqlX9JG/E1DSsfcULub+xkdkIHmzNynpHeR8F/Ki5Tckom70Jc5ykzLvUngk4H/ksfH4jsd6BqrpQYyAm64dt/lMIray2ZXMz4lVb2uWdptkOYZ97G7nOsub2CQ3Jr+S9zGFSqkSQvQ1PIboVsu69b+15pQirhiaHKKLZPko1WLA1MK1tVn0NrGm1ipOdZVIy5Cx8uMaAC1QGpcVXT0QV4cRp9aZZCV3Gwqg1jsU1O4YipsVIcwkLoJYpqUZ2aR6iBNjCn1kwJbdSjWzruB4b4i91zi+TMrITRr/UYM6+eTAY3sPiQjhUpr0eCvH70gYDKOUWzmKzfWQ1eiCAWZDV2vM2dUuPRzNSy6cSPjvZ2SYlUVb/h/WQU9327EE6lie7WGuRa2NrcyJWri2TCet45aBXs0PNK6Y0pbL0G8F1Ky80PwXyKamzsK87vWRXD41LVCBPzoC8OSxM36DMbrN9FLHfNuAWzoihKrPMSK7wHJRcOJfgeSg1MdTuSUcZaqBYaO+ZF9uouP5kYFttdledNc6j7IQTjb2unygXPeWgCDFiq3e7mb8TESQvkCltLW3DJ/HT2uhdwFl3vcbWhvJuV8mMx+snnRuphdaZmEz4Gsjf7YGGaNEMQKv93uJZs3+jTkTVJryY/LVmcdwkuiV2nUajI2TXRPGzfqUHYkKybtlEhDv0/oGgGaZslaMMOxbnvPOFLcFnaWaAQBt1+zLt+BnzEV2qLoOlPUxzwifNogRM0my/UVVIrzG9/fDeX4kXi1RBsdxIldDe37EJNgSKRZo7fYHsQjWTEjlEOcUtXHgug+ilGZGJbc36ZaHgVwZGi/BOZo0RUnSnRpFXMfgRoUPxmSSayLJLBf0KLxohvu9Lp7s1Pb0/l7dSYmDlX7SBj4fUvPnZKd8Wy8WFmC1UeBGx5UveV2ZHq5Bd3SWsAAsv2rMoQ0ecM+x39QVyjLaFuzapA1Y8KDnP8CH8DG2azRsZq+FjnivmFsIWyYmjPnp3fSxKnVSY2oYSs2rhjS2XyUF1m93V4XDKh+5r6kBVGVAndEFugShzcHe9lmaGgxAblFj9xW14TGCkguoHK2vlpqeUTeAl+kwnWX0C3ECUWnBnDdcLRtyd8vsGKZs0l6iyQomq7EM6hU6MTYZPXkhkF+mjCA+JnmBbszS7lkZOdQvDGdMnb3KkOYGEaja769ka0WWunCnBj3kPKjMDOOqsX6pLNrbMkPyu/vkZ0CFZk02ZsfwTHgOYvLtew7T0DYIiZWKfD5mLzuAlBRF7lSJjNtdvaOrm+s5Dn/vkD8n2rUES8YHQ1MGa5WseFlxH9lirHsVb2MiI9Dg2W3oR2SD2f+imC5POZw+0IVtO7bOPgsbWepu8xepMmJo7fg8yJVcSW0N6zVWwcdyFMQPR90w9m91xRKBGqrUrgGmxEPC0XsQ6gz84MMYnr5piDRuzWVnHbziVih6nyJLRbAysu1C8YP6jD0lk5zRkYUpXCjmL7sS41bOvrZv1pf7kUgeKfKemU7v9Ah1CSrmEYVtStV8MByebt87THMwcqURl/jeP4BUtMFBT+HyJfEnSDDDQBmh98WPfHfRl2QTGRtvsnKls0LnT0SUn2/edCqe1vjmJbnB0mZF1mo72pGN3vOuXg6kLZ6pL2GKhngRbnkZ2xE4ZjD8B2mH35sgCduVUNu939bWnntTBTSO7qzb/+yoD+3YLtzvkdZVPSUyNQfTZgX5MQZ4Z6dDI3HRq7RRtKYQJqlj6NieBlUd2R6hmYsmklNhS68pZx8oM7W/mLuitBxJ6R+F+Qfh3dLi+hL7NvdDO+eSPsO0pdjLMJubI7iMl3M8cKuPXPzpcX2pUYo8Eyz+2ErATFeWVqWlGapifFJmDLno0GhyQbEsXy4JSF6ediNykWIv0fHZrCix47xdO8VNH5BfGjsGQd5Pi8QzJzbmXbrNDNy+0wbGlOoYRiZwhsWNNzmzRqUNWIkr+6uTfCJFpe8N6+jA3QdnoMczFU6wYW98H31hOXZZwCWPPzR0QdcoanRvpc/Xap89FPvcPkGuxASXWwKGdma7Ph3z0sSgTrtvWn2hOoSU6XCwTfOopGl5CI7fDHK3+P8gZsfovCmfEiuwZsSJ7RqzInhErsmfEiuwZ8V9MiM5/Tt1yaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (224, 224, 3) (1, 12)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABrCAYAAAAYTP40AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAJ1ElEQVR4nO2dXageRxnHf/9zEitWY1qtQdM28TtildYqihTshUUqBAvihVRQb/RGQShFEIrplSJeBLwoggGlkaqgiFTUUosfKCqYahE/QGhKtH62PTGNUZNzHi929mTOnt3ZmdnZ3fek7x/ew+7O1zP/efaZZ+aZ9z0yM5aYBitzC/BMwpLsCbEke0IsyZ4QS7InxJLsCbHjyZZ0RNLxueWIQTbZkm6S9FNJpyU9Keknkt5UUrgpIekXkl4p6WWSTnjPL5N0TNJjks5IeljSrTltZJEtaQ9wP/A54EpgP3A38N+c+uaGpN3AAeCPwI3ACS95F3AKeBvwfOAu4GuSDqa2k6vZrwIws/vMbN3MzpnZA2b2iBP+5ZIekvSEpH9K+rKkvXVhSScl3SnpEUlnnebsk/Qdpz0PSrrC5T0oySR9SNLjkv4i6Y4uwSS9xb1xa5J+LenmiP5cB/zWquX0G/HINrOzZnbEzE6a2YaZ3Q88SjUoaTCz5A+wB3gC+BJwK3BFI/0VwC3AZcBVwI+Ao176SeBnwD6qt+LvroM3uDIPAZ90eQ8CBtwHXA68DvgH8HaXfgQ47q73O7neSaVIt7j7qzr68UFgDfg38B93fQE4465f2lJmn8t7KJm3HLJdo68Bvgj8yQn4LWBfR97bgIcbZN/u3X8duMe7/yjwzQbZh7z0zwDHWsj+OHBvo+3vAe/v6cuPgeuBa4FfAerItxt4EPh8DmfZE6SZ/c7MPmBmV1O9hi8BjgJIepGkr0j6s6R/AceBFzaq+Jt3fa7l/rmN/Ke868dce00cAN7jTMiapDXgJuDFzYySrnR5TgNvBX4A/AF4NfCUpI818q8A9wL/Az7S0nYvirh+ZvZ7Ki2/zj36FJU2vt7M9gDvAzSwmWu862uBx1vynKLS7L3e53Iz+3SLzE+a2V7gw8AX3PV3gcOu3NE6ryQBx6hMyLvN7HxOB3K9kUOS7pB0tbu/BngvlR0GeB7wNLAmaT9wZ047Ddwl6TmSXktla7/akuc4cFjSOyStSnq2pJtrOTvgex83AL9syXMPldk8bGbncjuQq9lngDcDP5d0lork3wC1l3A38AbgNPBt4Bu5Anr4IZVr9n3gs2b2QDODmZ0C3gV8gmoSPUU10KF+3gickPQCYN3MnvITJR2g0v7rgb9Ketp9bk/tgGzBgwfOn30U2G1mF2YWZxB2/HJ9J2FJ9oRYeDNyKWGp2RNiSfaE2BVK3C1tMzJDVyap5ecycjnt1n07b9bazSDZbQ12CbGt9hVgI7781BhDjr46e82I6NZGhdJbiN7MuEq1pTMjpB75R0BQs33UAlnjPgWqC69TDUY91G0D04Ki2mhsefti+jO0/V6yuxpoey7/oiXDlkdtvesoNxo6BnksEcp4I+5dNNwnRtoNl7nu8AoXexkjVUE/yhqfsRBtRmCrMm4Rylxaqmb6eTc6rruwkdFeB1JM4pDmksj27XWzn+b+DJlstpT1vZnmhNF+OwmGjG8S2TVCjfW6hqtUE2RX2bo3vnbXz5pljeJ2fmHMSC62aGwH0Zto621tMtrKFman7c0s1cQgsrtseAm/tbWOGC9nBMT2p0+OQWRnuYW5dfsPOlanWxobYQS6+hXb1CRmBLYLlKz9/iTZ560UIDq2ipSmsl2/toY6nIbO8kmY2PXIlTUkZpbr15fe9WoNWeqnlptrwysk4yhmJHZQapSagILos/Njt0/PoleNTzNtobBKWKjafRwg+NAdwijN7jIJc9rwbWXXvYSulWa90xcSKqftSCQHD8ZIzxE+WLeq7dzOPDMZ9CDZfZpbCiX88k2b7F4565qlZ7Tbg1y/PrStKkdxC2HrLmAfIwMWPaPZ7FR5Qn0Y0y3cLJOwEpnDkhR1/WI6MIZbuK3OqSM+HrIXNUNf7UWJpG9ixkGAkfdGQoMV2oqWl54Fq6Ln2yqfYMcwewU5thL02fdBrmFH5XMqd9IKcq5V46BgbEOd5jRtyWakFOFNM9LUuGIDe4FqKV/713Uj9fJ+wuP1yWTnaEYzotN8S3wbbY18zXaTBsEdj1DbQmbdVdbYoNrxMci+eappo/3nrVF8h17iHZHmj1yz0vq8SpvPWZj52cyIj75ofbaG+yaijs7v4uKyvq58he3B5BEi95OFxULoIq7InokPR6hWuEjkeSqy3TGJLW3Ordk+SsoSs9Sv89XPskNX5h3ydPd+I2O5h0WOMpQQLCfAmk14vTfSsXE11iRZxIyM7X8PjV0GKy4QLotFcZudohWx5DVdwuazlLq2tZ9A9GZ7mXZm1gkyVd7QNm2R3cIQ6sXBgNOzxcnuOivXtO99ZHS5fLHtZEFUjNTfjGhW7ruDu0hefU5+sDLFdITuY8okw0DnXUUtp21t8w8V0YkaPhrZQ6M8zX2TtnpD3lC2W6huW75NjsROjkZ2qlsYssExobZi8ENmgbPkm0jQ7tHNyMId5knBOv3faFsEMxKD4svxljoHnytM9MGzY5ClkGLaYvKGPJGm+cr1xRduuR6LKUxJM265iOarN7q+cBHyALIITuhkzNoghMkDviXqTFkQ9c4LBTvZJ9fkE2SJncI+O+yblGhtDwVEW9rNQdYhnSm0c/L6e4gugaJf8xgDg/c7GnXV6KyzkP1uwyg2u6tcU8Dc1WUXWgnwNNU3YUP98c3yCfvho9js3M2mUdAyUkXarStJWPQMPli5k1zDoii9nz0lkbltZXkbPTJkR316sDA/NZdzlrBr2d6KOt4Y6HGSW9po3GifC3yMcj57yBsx1J4Gy9chrcCkFt3+GGak7e0b0/+eBDMJmvWDXKVWfzVKrCP6ojnbEnsCA6W2an0UPzfSR1qX0CXdwKi6egIDY7ilyb/rl5pvoU1L6uGcga9gNNlzkxaaPwbHIiNJNBum8XGu34LsxPfNH9kKkVDQVvLbidPsQO3ZewqJWITx7jyuJuBZVPNA4OBOHNmBQx0xu3Jzm6DRYVz8F3YBMvI028LJQxDaMcw5+DMmUueKMNkDZt9cryQk+CKYEh+t8gQ6GiY7kegcoi55E+OhzH62+8ZV81u1O8rnngBpZHeZlXqDx8cKmP9zzZmn+9tCg31Hicc4aVUCaWSHVLOZtsHFL+tHHiDvI6krkh7zbBGQvp+d0JMtZiVAdN8+8KWCeM32f4PJv+/B3G5hW0BgLs2PJ9u3y7VpaENEuCNll7CrXEpaTPoUSPOzC/ncsWk70rQMXkG2VRiKAqy6e7c5H7MRvyOJbUP2oqbLJHQd1Woeiu74xYMSxIbqiD5UOTHCZMf+IHhzsoyYOIceTI8pvwh22sfy/0FOiIU5N/JMwJLsCbEke0IsyZ4QS7InxJLsCfF/TXYRvd3PTX8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (224, 224, 3) (1, 12)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sjg_train_dataset = SeomjingangDataset(csv_file='new_train_output_seomjingang.csv',\n",
    "                                 root_dir='data/train/',\n",
    "                                 time_image_dict = time_image_dict)\n",
    "sjg_val_dataset = SeomjingangDataset(csv_file='new_train_output_seomjingang.csv',\n",
    "                                 root_dir='data/train/',\n",
    "                                 time_image_dict = time_image_dict)\n",
    "\n",
    "# check dataset\n",
    "fig = plt.figure()\n",
    "\n",
    "def show_tpw(image):\n",
    "    \"\"\"Show image tpw\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "print(sjg_train_dataset)\n",
    "for i in range(len(sjg_train_dataset)):\n",
    "    sample = sjg_train_dataset[i]\n",
    "\n",
    "    print(i, sample['image'].shape, sample['hourly_rainfall'].shape)\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    show_tpw(sample['image'])\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "edZgyw5PsW4j",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Transform image to make it fit to efficientNet\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, tpws = sample['image'], sample['hourly_rainfall']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        return {'image': img, 'hourly_rainfall': tpws}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, tpws = sample['image'], sample['hourly_rainfall']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'hourly_rainfall': torch.from_numpy(tpws)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nT8rzJ4L_oSl",
    "outputId": "d26c4d65-4606-4a9c-81f7-20fea43cc67a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_dict length 11498\n",
      "image_dict length 11664\n",
      "image_dict length 11498\n",
      "image_dict length 11664\n",
      "0 torch.Size([3, 224, 224]) torch.Size([1, 12])\n",
      "1 torch.Size([3, 224, 224]) torch.Size([1, 12])\n",
      "2 torch.Size([3, 224, 224]) torch.Size([1, 12])\n",
      "3 torch.Size([3, 224, 224]) torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sjg_train_dataset = SeomjingangDataset(csv_file='new_train_output_seomjingang.csv',\n",
    "                                 root_dir='data/train/',\n",
    "                                 time_image_dict = time_image_dict,\n",
    "                                 transform=transforms.Compose([\n",
    "                                               Rescale(224),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "sjg_val_dataset = SeomjingangDataset(csv_file='new_train_output_seomjingang.csv',\n",
    "                                 root_dir='data/train/',\n",
    "                                 time_image_dict = time_image_dict,\n",
    "                                 transform=transforms.Compose([\n",
    "                                               Rescale(224),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "for i in range(len(sjg_train_dataset)):\n",
    "    sample = sjg_train_dataset[i]\n",
    "\n",
    "    print(i, sample['image'].size(), sample['hourly_rainfall'].size())\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "U_WPXoAcU8ac",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "#torch.cuda.empty_cache()\n",
    "N = len(sjg_train_dataset)\n",
    "N_split = int(N * 0.7)\n",
    "\n",
    "# Random split\n",
    "train_set_size = int(len(sjg_train_dataset) * 0.7)\n",
    "valid_set_size = len(sjg_train_dataset) - train_set_size\n",
    "indices = list(range(N))\n",
    "\n",
    "train_idx, valid_idx = indices[:train_set_size], indices[train_set_size:]\n",
    "train_set = torch.utils.data.Subset(sjg_train_dataset, train_idx)\n",
    "valid_set = torch.utils.data.Subset(sjg_train_dataset, valid_idx)\n",
    "\n",
    "# make dataloader\n",
    "train_dl = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_dl = DataLoader(valid_set, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSpYOxOH9TVt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. 모델 구축하기\n",
    "코드는 https://github.com/zsef123/EfficientNets-PyTorch/blob/master/models/effnet.py 를 참고했습니다.\n",
    "\n",
    "https://github.com/katsura-jp/efficientnet-pytorch/blob/master/model/efficientnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjJoT-8v9g9P",
    "outputId": "81b18417-dbf7-44fb-82e1-8c9e3e794bc2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([3, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Swish activation function\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(3, 3, 224, 224)\n",
    "    model = Swish()\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uixhxPQ6GpT4",
    "outputId": "7a6f4540-0630-4971-9131-0e630e4b7c50",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([3, 56, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# SE Block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, r=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels * r),\n",
    "            Swish(),\n",
    "            nn.Linear(in_channels * r, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.excitation(x)\n",
    "        x = x.view(x.size(0), x.size(1), 1, 1)\n",
    "        return x\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(3, 56, 17, 17)\n",
    "    model = SEBlock(x.size(1))\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zq70pzaKVxO",
    "outputId": "b359ad36-72b7-4d59-df33-971fe228d82e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([3, 16, 24, 24]) Stochastic depth: tensor(False)\n"
     ]
    }
   ],
   "source": [
    "class MBConv(nn.Module):\n",
    "    expand = 6\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
    "        super().__init__()\n",
    "        # first MBConv is not using stochastic depth\n",
    "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * MBConv.expand, 1, stride=stride, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_channels * MBConv.expand, in_channels * MBConv.expand, kernel_size=kernel_size,\n",
    "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*MBConv.expand),\n",
    "            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(in_channels * MBConv.expand, se_scale)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*MBConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stochastic depth\n",
    "        if self.training:\n",
    "            if not torch.bernoulli(self.p):\n",
    "                return x\n",
    "\n",
    "        x_shortcut = x\n",
    "        x_residual = self.residual(x)\n",
    "        x_se = self.se(x_residual)\n",
    "\n",
    "        x = x_se * x_residual\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.shortcut:\n",
    "            x= x_shortcut + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(3, 16, 24, 24)\n",
    "    model = MBConv(x.size(1), x.size(1), 3, stride=1, p=1)\n",
    "    model.train()\n",
    "    output = model(x)\n",
    "    x = (output == x)\n",
    "    print('output size:', output.size(), 'Stochastic depth:', x[1,0,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbmQXf2aZ3FZ",
    "outputId": "311984e3-a973-4b73-fc41-d47e4309359b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([3, 16, 24, 24]) Stochastic depth: tensor(False)\n"
     ]
    }
   ],
   "source": [
    "class SepConv(nn.Module):\n",
    "    expand = 1\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
    "        super().__init__()\n",
    "        # first SepConv is not using stochastic depth\n",
    "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * SepConv.expand, in_channels * SepConv.expand, kernel_size=kernel_size,\n",
    "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*SepConv.expand),\n",
    "            nn.BatchNorm2d(in_channels * SepConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(in_channels * SepConv.expand, se_scale)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*SepConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stochastic depth\n",
    "        if self.training:\n",
    "            if not torch.bernoulli(self.p):\n",
    "                return x\n",
    "\n",
    "        x_shortcut = x\n",
    "        x_residual = self.residual(x)\n",
    "        x_se = self.se(x_residual)\n",
    "\n",
    "        x = x_se * x_residual\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.shortcut:\n",
    "            x= x_shortcut + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(3, 16, 24, 24)\n",
    "    model = SepConv(x.size(1), x.size(1), 3, stride=1, p=1)\n",
    "    model.train()\n",
    "    output = model(x)\n",
    "    # stochastic depth check\n",
    "    x = (output == x)\n",
    "    print('output size:', output.size(), 'Stochastic depth:', x[1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxPq3eBKGGRM",
    "outputId": "2079aa57-23c8-4ad6-bb15-73289515e39d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# class EfficientNet(nn.Module):\n",
    "#     def __init__(self, num_classes=10, width_coef=1., depth_coef=1., scale=1., dropout=0.2, se_scale=4, stochastic_depth=False, p=0.5):\n",
    "#         super().__init__()\n",
    "#         channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
    "#         repeats = [1, 2, 2, 3, 3, 4, 1]\n",
    "#         strides = [1, 2, 2, 2, 1, 2, 1]\n",
    "#         kernel_size = [3, 3, 5, 3, 5, 5, 3]\n",
    "#         depth = depth_coef\n",
    "#         width = width_coef\n",
    "\n",
    "#         channels = [int(x*width) for x in channels]\n",
    "#         repeats = [int(x*depth) for x in repeats]\n",
    "\n",
    "#         # stochastic depth\n",
    "#         if stochastic_depth:\n",
    "#             self.p = p\n",
    "#             self.step = (1 - 0.5) / (sum(repeats) - 1)\n",
    "#         else:\n",
    "#             self.p = 1\n",
    "#             self.step = 0\n",
    "\n",
    "\n",
    "#         # efficient net\n",
    "#         self.upsample = nn.Upsample(scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "\n",
    "#         self.stage1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, channels[0],3, stride=2, padding=1, bias=False),\n",
    "#             nn.BatchNorm2d(channels[0], momentum=0.99, eps=1e-3)\n",
    "#         )\n",
    "\n",
    "#         self.stage2 = self._make_Block(SepConv, repeats[0], channels[0], channels[1], kernel_size[0], strides[0], se_scale)\n",
    "\n",
    "#         self.stage3 = self._make_Block(MBConv, repeats[1], channels[1], channels[2], kernel_size[1], strides[1], se_scale)\n",
    "\n",
    "#         self.stage4 = self._make_Block(MBConv, repeats[2], channels[2], channels[3], kernel_size[2], strides[2], se_scale)\n",
    "\n",
    "#         self.stage5 = self._make_Block(MBConv, repeats[3], channels[3], channels[4], kernel_size[3], strides[3], se_scale)\n",
    "\n",
    "#         self.stage6 = self._make_Block(MBConv, repeats[4], channels[4], channels[5], kernel_size[4], strides[4], se_scale)\n",
    "\n",
    "#         self.stage7 = self._make_Block(MBConv, repeats[5], channels[5], channels[6], kernel_size[5], strides[5], se_scale)\n",
    "\n",
    "#         self.stage8 = self._make_Block(MBConv, repeats[6], channels[6], channels[7], kernel_size[6], strides[6], se_scale)\n",
    "\n",
    "#         self.stage9 = nn.Sequential(\n",
    "#             nn.Conv2d(channels[7], channels[8], 1, stride=1, bias=False),\n",
    "#             nn.BatchNorm2d(channels[8], momentum=0.99, eps=1e-3),\n",
    "#             Swish()\n",
    "#         ) \n",
    "\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "#         #self.linear = nn.Linear(channels[8], num_classes) # TODO: change here to 1?\n",
    "#         self.linear = nn.Linear(channels[8], 12)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.upsample(x)\n",
    "#         x = self.stage1(x)\n",
    "#         x = self.stage2(x)\n",
    "#         x = self.stage3(x)\n",
    "#         x = self.stage4(x)\n",
    "#         x = self.stage5(x)\n",
    "#         x = self.stage6(x)\n",
    "#         x = self.stage7(x)\n",
    "#         x = self.stage8(x)\n",
    "#         x = self.stage9(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.linear(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "#     def _make_Block(self, block, repeats, in_channels, out_channels, kernel_size, stride, se_scale):\n",
    "#         strides = [stride] + [1] * (repeats - 1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(in_channels, out_channels, kernel_size, stride, se_scale, self.p))\n",
    "#             in_channels = out_channels\n",
    "#             self.p -= self.step\n",
    "\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# def efficientnet_b0(num_classes=10):\n",
    "#     return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.0, scale=1.0,dropout=0.2, se_scale=4)\n",
    "\n",
    "# def efficientnet_b1(num_classes=10):\n",
    "#     return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.1, scale=240/224, dropout=0.2, se_scale=4)\n",
    "\n",
    "# def efficientnet_b2(num_classes=10):\n",
    "#     return EfficientNet(num_classes=num_classes, width_coef=1.1, depth_coef=1.2, scale=260/224., dropout=0.3, se_scale=4)\n",
    "\n",
    "# def efficientnet_b3(num_classes=10):\n",
    "#     return EfficientNet(num_classes=num_classes, width_coef=1.2, depth_coef=1.4, scale=300/224, dropout=0.3, se_scale=4)\n",
    "\n",
    "# def efficientnet_b4(num_classes=10):\n",
    "#     return EfficientNet(num_classes=num_classes, width_coef=1.4, depth_coef=1.8, scale=380/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "# def efficientnet_b5(num_classes=10):\n",
    "#     return EfficientNet(num_classes=num_classes, width_coef=1.6, depth_coef=2.2, scale=456/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "# def efficientnet_b6(num_classes=10):\n",
    "#     return EfficientNet(num_classes=num_classes, width_coef=1.8, depth_coef=2.6, scale=528/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "# def efficientnet_b7(num_classes=10):\n",
    "#     return EfficientNet(num_classes=num_classes, width_coef=2.0, depth_coef=3.1, scale=600/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "\n",
    "# # check\n",
    "# if __name__ == '__main__':\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     x = torch.randn(3, 3, 224, 224).to(device)\n",
    "#     model = efficientnet_b0().to(device)\n",
    "#     output = model(x)\n",
    "#     print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, width_coef=1., depth_coef=1., scale=1., dropout=0.2, se_scale=4, stochastic_depth=False, p=0.5):\n",
    "        super().__init__()\n",
    "        channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
    "        repeats = [1, 2, 2, 3, 3, 4, 1]\n",
    "        strides = [1, 2, 2, 2, 1, 2, 1]\n",
    "        kernel_size = [3, 3, 5, 3, 5, 5, 3]\n",
    "        depth = depth_coef\n",
    "        width = width_coef\n",
    "\n",
    "        channels = [int(x*width) for x in channels]\n",
    "        repeats = [int(x*depth) for x in repeats]\n",
    "\n",
    "        # stochastic depth\n",
    "        if stochastic_depth:\n",
    "            self.p = p\n",
    "            self.step = (1 - 0.5) / (sum(repeats) - 1)\n",
    "        else:\n",
    "            self.p = 1\n",
    "            self.step = 0\n",
    "\n",
    "\n",
    "        # efficient net\n",
    "        self.upsample = nn.Upsample(scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(3, channels[0],3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(channels[0], momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.stage2 = self._make_Block(SepConv, repeats[0], channels[0], channels[1], kernel_size[0], strides[0], se_scale)\n",
    "\n",
    "        self.stage3 = self._make_Block(MBConv, repeats[1], channels[1], channels[2], kernel_size[1], strides[1], se_scale)\n",
    "\n",
    "        self.stage4 = self._make_Block(MBConv, repeats[2], channels[2], channels[3], kernel_size[2], strides[2], se_scale)\n",
    "\n",
    "        self.stage5 = self._make_Block(MBConv, repeats[3], channels[3], channels[4], kernel_size[3], strides[3], se_scale)\n",
    "\n",
    "        self.stage6 = self._make_Block(MBConv, repeats[4], channels[4], channels[5], kernel_size[4], strides[4], se_scale)\n",
    "\n",
    "        self.stage7 = self._make_Block(MBConv, repeats[5], channels[5], channels[6], kernel_size[5], strides[5], se_scale)\n",
    "\n",
    "        self.stage8 = self._make_Block(MBConv, repeats[6], channels[6], channels[7], kernel_size[6], strides[6], se_scale)\n",
    "\n",
    "        self.stage9 = nn.Sequential(\n",
    "            nn.Conv2d(channels[7], channels[8], 1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(channels[8], momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        #self.linear = nn.Linear(channels[8], num_classes) # TODO: change here to 1?\n",
    "        self.linear = nn.Linear(channels[8], 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.stage8(x)\n",
    "        x = self.stage9(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _make_Block(self, block, repeats, in_channels, out_channels, kernel_size, stride, se_scale):\n",
    "        strides = [stride] + [1] * (repeats - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(in_channels, out_channels, kernel_size, stride, se_scale, self.p))\n",
    "            in_channels = out_channels\n",
    "            self.p -= self.step\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def efficientnet_b0(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.0, scale=1.0,dropout=0.2, se_scale=4)\n",
    "\n",
    "def efficientnet_b1(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.1, scale=240/224, dropout=0.2, se_scale=4)\n",
    "\n",
    "def efficientnet_b2(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.1, depth_coef=1.2, scale=260/224., dropout=0.3, se_scale=4)\n",
    "\n",
    "def efficientnet_b3(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.2, depth_coef=1.4, scale=300/224, dropout=0.3, se_scale=4)\n",
    "\n",
    "def efficientnet_b4(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.4, depth_coef=1.8, scale=380/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "def efficientnet_b5(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.6, depth_coef=2.2, scale=456/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "def efficientnet_b6(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.8, depth_coef=2.6, scale=528/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "def efficientnet_b7(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=2.0, depth_coef=3.1, scale=600/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "       \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "       \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "       \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "       \n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "       \n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        h_out = h_out.view(-1, self.hidden_size)\n",
    "       \n",
    "        out = self.fc(h_out)\n",
    "       \n",
    "        return out\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "       \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #internal state\n",
    "        # Propagate input through LSTM\n",
    "       \n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "       \n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "       \n",
    "        return out\n",
    "   \n",
    "from torch.autograd import Variable\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    x = torch.randn(3, 3, 224, 224).to(device)\n",
    "    input_size = 12 #number of features\n",
    "    hidden_size = 4 #number of features in hidden state\n",
    "    num_layers = 2 #number of stacked lstm layers\n",
    "\n",
    "    num_classes = 12 #number of output classes\n",
    "    lstm = LSTM(num_classes, input_size, hidden_size, num_layers, x).to(device)\n",
    "    model = efficientnet_b0().to(device)\n",
    "    model2 = nn.Sequential(\n",
    "        model,\n",
    "        lstm\n",
    "    )\n",
    "    output = model2(x)\n",
    "    print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "a1X1ZICfetfq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print model summary\n",
    "model = efficientnet_b0().to(device)\n",
    "#summary(model, (3,224,224), device=device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYiTiNkJwS5p",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eUmVhxGImFsX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define loss function, optimizer, lr_scheduler\n",
    "#loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "\n",
    "# get current lr\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "# this metric is for classification!\n",
    "# calculate the metric per mini-batch\n",
    "def metric_batch(output, target):\n",
    "    # original function returns number of correct classification\n",
    "    # by doing so, measures accuracy\n",
    "    # instead, calculate how close each value is to target value,\n",
    "    # check whether it is within some threshold\n",
    "    # and count number of items within the threshold\n",
    "    # threshold: parameter to fine-tune\n",
    "    correct_threshold = 0.01; #TODO change this?\n",
    "    \n",
    "    diff = abs(output - target)\n",
    "    corrects = torch.count_nonzero(diff.le(correct_threshold)).item()\n",
    "    #pred = output.argmax(1, keepdim=True) # index of max value in output\n",
    "    #corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "# calculate the loss per mini-batch\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss_b = loss_func(output, target).float()\n",
    "    metric_b = metric_batch(output, target)\n",
    "\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss_b.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    return loss_b.item(), metric_b\n",
    "\n",
    "\n",
    "# calculate the loss per epochs\n",
    "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "\n",
    "    for val in dataset_dl:\n",
    "        xb = val['image']\n",
    "        yb = torch.squeeze(val['hourly_rainfall']).float()\n",
    "        xb = xb.to(device).float()\n",
    "        yb = yb.to(device).float()\n",
    "        output = model(xb).float()\n",
    "        \n",
    "        assert(output.shape == yb.shape, \"shapes do not match!\")\n",
    "\n",
    "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
    "\n",
    "        running_loss += loss_b\n",
    "        \n",
    "        if metric_b is not None:\n",
    "            running_metric += metric_b\n",
    "\n",
    "        if sanity_check is True:\n",
    "            break\n",
    "\n",
    "    loss = running_loss / len_data\n",
    "    metric = running_metric / len_data\n",
    "    return loss, metric\n",
    "\n",
    "\n",
    "# function to start training\n",
    "def train_val(model, params):\n",
    "    num_epochs=params['num_epochs']\n",
    "    loss_func=params['loss_func']\n",
    "    opt=params['optimizer']\n",
    "    train_dl=params['train_dl']\n",
    "    val_dl=params['val_dl']\n",
    "    sanity_check=params['sanity_check']\n",
    "    lr_scheduler=params['lr_scheduler']\n",
    "    path2weights=params['path2weights']\n",
    "\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    metric_history = {'train': [], 'val': []}\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr= {}'.format(epoch, num_epochs-1, current_lr))\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        metric_history['train'].append(train_metric)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
    "        loss_history['val'].append(val_loss)\n",
    "        metric_history['val'].append(val_metric)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print('Copied best model weights!')\n",
    "\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print('Loading best model weights!')\n",
    "            model.load_state_dict(best_model_wts)\n",
    "\n",
    "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
    "        print('-'*10)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IsuU2EiLwZVz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define the training parameters\n",
    "params_train = {\n",
    "    'num_epochs':100,\n",
    "    'optimizer':opt,\n",
    "    'loss_func':loss_func,\n",
    "    'train_dl':train_dl,\n",
    "    'val_dl':val_dl,\n",
    "    'sanity_check':False,\n",
    "    'lr_scheduler':lr_scheduler,\n",
    "    'path2weights':'./models/weights.pt',\n",
    "}\n",
    "\n",
    "# check the directory to save weights.pt\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSerror:\n",
    "        print('Error')\n",
    "#createFolder('/content/drive/MyDrive/data/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3yS8vS_wwbUG",
    "outputId": "35e0adec-70fa-4dea-b8cc-91acfba106fc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99, current lr= 0.01\n",
      "train loss: nan, val loss: nan, accuracy: 0.00, time: 2.7232 min\n",
      "----------\n",
      "Epoch 1/99, current lr= 0.01\n",
      "train loss: nan, val loss: nan, accuracy: 0.00, time: 5.3136 min\n",
      "----------\n",
      "Epoch 2/99, current lr= 0.01\n"
     ]
    }
   ],
   "source": [
    "model, loss_hist, metric_hist = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3v79XI6YxG_U",
    "outputId": "e66f47a1-04df-4cde-aa3e-c80b613f26a3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = params_train['num_epochs']\n",
    "\n",
    "# Plot train-val loss\n",
    "plt.title('Train-Val Loss')\n",
    "plt.plot(range(1, num_epochs+1), loss_hist['train'], label='train')\n",
    "plt.plot(range(1, num_epochs+1), loss_hist['val'], label='val')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Training Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot train-val accuracy\n",
    "plt.title('Train-Val Accuracy')\n",
    "plt.plot(range(1, num_epochs+1), metric_hist['train'], label='train')\n",
    "plt.plot(range(1, num_epochs+1), metric_hist['val'], label='val')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Training Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfwTbLHqyS55",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ai-challenge 20211114.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}